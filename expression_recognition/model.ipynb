{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd8de5f",
   "metadata": {
    "papermill": {
     "duration": 0.015404,
     "end_time": "2024-07-08T15:19:47.480642",
     "exception": false,
     "start_time": "2024-07-08T15:19:47.465238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28adca84",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-08T15:19:47.513523Z",
     "iopub.status.busy": "2024-07-08T15:19:47.513115Z",
     "iopub.status.idle": "2024-07-08T15:20:40.179912Z",
     "shell.execute_reply": "2024-07-08T15:20:40.178618Z"
    },
    "papermill": {
     "duration": 52.68682,
     "end_time": "2024-07-08T15:20:40.183003",
     "exception": false,
     "start_time": "2024-07-08T15:19:47.496183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in e:\\anaconda\\envs\\llm\\lib\\site-packages (0.10.21)\n",
      "Requirement already satisfied: absl-py in e:\\anaconda\\envs\\llm\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in e:\\anaconda\\envs\\llm\\lib\\site-packages (from mediapipe) (24.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in e:\\anaconda\\envs\\llm\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in e:\\anaconda\\envs\\llm\\lib\\site-packages (from mediapipe) (0.5.0)\n",
      "Requirement already satisfied: jaxlib in e:\\anaconda\\envs\\llm\\lib\\site-packages (from mediapipe) (0.5.0)\n",
      "Requirement already satisfied: matplotlib in e:\\anaconda\\envs\\llm\\lib\\site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: numpy<2 in e:\\anaconda\\envs\\llm\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in e:\\anaconda\\envs\\llm\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in e:\\anaconda\\envs\\llm\\lib\\site-packages (from mediapipe) (4.25.6)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in e:\\anaconda\\envs\\llm\\lib\\site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in e:\\anaconda\\envs\\llm\\lib\\site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in e:\\anaconda\\envs\\llm\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in e:\\anaconda\\envs\\llm\\lib\\site-packages (from jax->mediapipe) (0.4.1)\n",
      "Requirement already satisfied: opt_einsum in e:\\anaconda\\envs\\llm\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in e:\\anaconda\\envs\\llm\\lib\\site-packages (from jax->mediapipe) (1.14.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in e:\\anaconda\\envs\\llm\\lib\\site-packages (from matplotlib->mediapipe) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\anaconda\\envs\\llm\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\anaconda\\envs\\llm\\lib\\site-packages (from matplotlib->mediapipe) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in e:\\anaconda\\envs\\llm\\lib\\site-packages (from matplotlib->mediapipe) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\anaconda\\envs\\llm\\lib\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in e:\\anaconda\\envs\\llm\\lib\\site-packages (from matplotlib->mediapipe) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in e:\\anaconda\\envs\\llm\\lib\\site-packages (from matplotlib->mediapipe) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in e:\\anaconda\\envs\\llm\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in e:\\anaconda\\envs\\llm\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in e:\\anaconda\\envs\\llm\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mediapipe as mp \n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27b8858",
   "metadata": {
    "papermill": {
     "duration": 0.01863,
     "end_time": "2024-07-08T15:20:40.220615",
     "exception": false,
     "start_time": "2024-07-08T15:20:40.201985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Mediapipe Facemesh\n",
    "\n",
    "Set up functionalities for detecting facial landmarks using MediaPipe, including defining specific facial feature sets like eyes, eyebrows, lips, and contours. We also initialize a configuration for detecting these landmarks in static images with refined precision and a minimum confidence threshold for detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87496367",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:20:40.260947Z",
     "iopub.status.busy": "2024-07-08T15:20:40.260184Z",
     "iopub.status.idle": "2024-07-08T15:20:40.283328Z",
     "shell.execute_reply": "2024-07-08T15:20:40.281121Z"
    },
    "papermill": {
     "duration": 0.049735,
     "end_time": "2024-07-08T15:20:40.289386",
     "exception": false,
     "start_time": "2024-07-08T15:20:40.239651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "LEFT_EYE = list(set(itertools.chain(*mp_face_mesh.FACEMESH_LEFT_EYE)))\n",
    "RIGHT_EYE = list(set(itertools.chain(*mp_face_mesh.FACEMESH_RIGHT_EYE)))\n",
    "LEFT_EYEBROW = list(set(itertools.chain(*mp_face_mesh.FACEMESH_LEFT_EYEBROW)))\n",
    "RIGHT_EYEBROW = list(set(itertools.chain(*mp_face_mesh.FACEMESH_RIGHT_EYEBROW)))\n",
    "LIPS = list(set(itertools.chain(*mp_face_mesh.FACEMESH_LIPS)))\n",
    "CONTOURS = list(set(itertools.chain(*mp_face_mesh.FACEMESH_CONTOURS)))\n",
    "OTHER = [1]\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3f6bd8",
   "metadata": {
    "papermill": {
     "duration": 0.019227,
     "end_time": "2024-07-08T15:20:40.333788",
     "exception": false,
     "start_time": "2024-07-08T15:20:40.314561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Extract face features\n",
    "\n",
    "Calculate Euclidean distances between facial landmarks in images for emotion detection then initializes a DataFrame to store distances in 2D and 3D from selected facial features like eyes, eyebrows, and lips, processed using MediaPipe's face mesh detection capabilities. We also limit to 100 samples for each emotion in order to have a faster processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a91a88a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:20:40.374332Z",
     "iopub.status.busy": "2024-07-08T15:20:40.373903Z",
     "iopub.status.idle": "2024-07-08T15:20:58.641345Z",
     "shell.execute_reply": "2024-07-08T15:20:58.640265Z"
    },
    "papermill": {
     "duration": 18.290934,
     "end_time": "2024-07-08T15:20:58.644121",
     "exception": false,
     "start_time": "2024-07-08T15:20:40.353187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def euc2d(a, b):\n",
    "    return np.sqrt((a[0]-b[0])*(a[0]-b[0]) + (a[1]-b[1])*(a[1]-b[1]))\n",
    "\n",
    "def euc3d(a, b):\n",
    "    return np.sqrt((a[0]-b[0])*(a[0]-b[0]) + (a[1]-b[1])*(a[1]-b[1]) + (a[2]-b[2])*(a[2]-b[2]))\n",
    "\n",
    "emotions = os.listdir('final_data/1/train')\n",
    "face_features = pd.DataFrame({}, columns=[f\"{i}\" for i in range(92 * 2)] + [\"y\"])\n",
    "\n",
    "for i, emotion in enumerate(emotions):\n",
    "    images = os.listdir(f'final_data/1/train/{emotion}')\n",
    "    selected_images = random.sample(images, 100)\n",
    "    for image in selected_images:\n",
    "        img = cv2.imread(f\"final_data/1/train/{emotion}/{image}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.GaussianBlur(img, (3, 3), cv2.BORDER_DEFAULT)\n",
    "        results = face_mesh.process(img)\n",
    "        if results.multi_face_landmarks:\n",
    "            shape = [(lmk.x, lmk.y, lmk.z) for lmk in results.multi_face_landmarks[0].landmark]\n",
    "            shape = np.array(shape)\n",
    "            nose = shape[1]\n",
    "            shape = shape[LEFT_EYE + RIGHT_EYE + LEFT_EYEBROW + RIGHT_EYEBROW + LIPS]\n",
    "            distances2d = [round(euc2d(nose, x), 6) for x in shape]\n",
    "            distances3d = [round(euc3d(nose, x), 6) for x in shape]\n",
    "            face_features.loc[len(face_features)] = distances2d + distances3d + [i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35053acd",
   "metadata": {
    "papermill": {
     "duration": 0.01907,
     "end_time": "2024-07-08T15:20:58.682820",
     "exception": false,
     "start_time": "2024-07-08T15:20:58.663750",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocess Dataset\n",
    "\n",
    "Shuffls the training data, standardizes the features using StandardScaler, converts the labels to categorical format, and reshapes the feature array to add an extra dimension for compatibility with MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfce8c22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:20:58.724906Z",
     "iopub.status.busy": "2024-07-08T15:20:58.724437Z",
     "iopub.status.idle": "2024-07-08T15:20:58.742137Z",
     "shell.execute_reply": "2024-07-08T15:20:58.740969Z"
    },
    "papermill": {
     "duration": 0.042128,
     "end_time": "2024-07-08T15:20:58.744962",
     "exception": false,
     "start_time": "2024-07-08T15:20:58.702834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "face_features = shuffle(face_features)\n",
    "X = face_features.iloc[:, :-1].values\n",
    "y = face_features.iloc[:, -1].values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X)\n",
    "y_train = to_categorical(y)\n",
    "X_train = X_train[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527070c",
   "metadata": {
    "papermill": {
     "duration": 0.01908,
     "end_time": "2024-07-08T15:20:58.783496",
     "exception": false,
     "start_time": "2024-07-08T15:20:58.764416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model\n",
    "\n",
    "Define a Sequential model architecture with layers of 128 and 64 neurons, and use ReLU activation for feature extraction. We also use the dropout regularization to prevent overfitting, with a final output layer using softmax activation for multi-class classification. The model is compiled with the Adam optimizer, categorical cross-entropy loss function, and accuracy metrics, trained over 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc3f6310",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:20:58.824808Z",
     "iopub.status.busy": "2024-07-08T15:20:58.824377Z",
     "iopub.status.idle": "2024-07-08T15:21:01.670399Z",
     "shell.execute_reply": "2024-07-08T15:21:01.669302Z"
    },
    "papermill": {
     "duration": 2.870062,
     "end_time": "2024-07-08T15:21:01.673220",
     "exception": false,
     "start_time": "2024-07-08T15:20:58.803158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2062 - loss: 2.1206\n",
      "Epoch 2/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3057 - loss: 1.8294\n",
      "Epoch 3/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3387 - loss: 1.7345\n",
      "Epoch 4/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3706 - loss: 1.6479\n",
      "Epoch 5/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4492 - loss: 1.5743\n",
      "Epoch 6/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step - accuracy: 0.4414 - loss: 1.5398\n",
      "Epoch 7/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4600 - loss: 1.5127\n",
      "Epoch 8/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4763 - loss: 1.4835\n",
      "Epoch 9/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4628 - loss: 1.4851\n",
      "Epoch 10/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4749 - loss: 1.4146\n",
      "Epoch 11/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5251 - loss: 1.3848\n",
      "Epoch 12/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5694 - loss: 1.3232\n",
      "Epoch 13/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5213 - loss: 1.3309\n",
      "Epoch 14/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5780 - loss: 1.2824\n",
      "Epoch 15/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5708 - loss: 1.2478\n",
      "Epoch 16/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5664 - loss: 1.2616\n",
      "Epoch 17/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5752 - loss: 1.2750\n",
      "Epoch 18/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6125 - loss: 1.1883\n",
      "Epoch 19/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5689 - loss: 1.2156 \n",
      "Epoch 20/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5708 - loss: 1.2001\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(emotions), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1028436,
     "sourceId": 1732825,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 80.232599,
   "end_time": "2024-07-08T15:21:04.397170",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-08T15:19:44.164571",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
